{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.datetime.now()\n",
    "print(\"current time:-\", ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get companies and reformat name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of company names and replace space,& for URL\n",
    "df_company = pd.read_excel('../../DataSet/CompanyList_Original.xlsx')\n",
    "all_names = df_company.Name.unique()\n",
    "print('Total companies : ' + str(len(all_names)))\n",
    "\n",
    "#Get list of company names and replace space,& for URL\n",
    "df = pd.read_excel('../../DataSet/Final.xlsx')\n",
    "scrapped_names = df.company.unique()\n",
    "print('Companies scraped : ' + str(len(scrapped_names)))\n",
    "\n",
    "rem_names = [c for c in all_names if c not in scrapped_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "sample = random.sample(rem_names, 200)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of company names and replace space,& for URL\n",
    "name_url = []\n",
    "for i in sample:\n",
    "    k = i.replace(\" \",\"+\").replace(\"&\",\"%26\").replace(\"'\",\"%27\")\n",
    "    name_url.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to extract data from Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see if the element exists\n",
    "def check_exists_by_xpath(xpath):\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Static Xpaths\n",
    "loadMoreXpath = str(\"//div[@class='search-result-more']/div[@class='search-result-more-txt']\")\n",
    "loadNoMoreXpath = str(\"//div[@class='search-result-more']/div[@class='search-result-more-txt search-result-no-more']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_to_excel(excel_path, companies_list, start_index, end_index):\n",
    "    writer = pd.ExcelWriter(excel_path, engine='openpyxl')\n",
    "    for company in companies_list[start_index:end_index]:\n",
    "        #Give chrome driver path\n",
    "        chromedriver = \"/usr/local/bin/chromedriver\"\n",
    "        os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "        driver2 = webdriver.Chrome(chromedriver)\n",
    "        url = \"https://www.reuters.com/search/news?blob=\"+company+\"&sortBy=date&dateRange=pastYear\"    \n",
    "        driver2.get(url)\n",
    "        \n",
    "        is_valid = True\n",
    "        for count in range(100):\n",
    "            value = check_exists_by_xpath(loadNoMoreXpath)\n",
    "            if value == True:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    loadMoreButton = driver2.find_element_by_xpath(loadMoreXpath)\n",
    "                except:\n",
    "                    is_valid = False\n",
    "                    break\n",
    "                loadMoreButton.click()\n",
    "                time.sleep(8)\n",
    "                count=count+1\n",
    "                print(\"CLICKED :\"+str(count))\n",
    "                \n",
    "        if not is_valid: continue\n",
    "        \n",
    "        titles=[]\n",
    "        links=[]\n",
    "        excerpts=[]\n",
    "        timestamps=[]\n",
    "\n",
    "        news_titles = driver2.find_elements_by_xpath(\".//div[@class='search-result-content']/h3[@class='search-result-title']/a[@href]\")\n",
    "        news_links = driver2.find_elements_by_xpath(\".//div[@class='search-result-content']/h3[@class='search-result-title']/a[@href]\")\n",
    "        news_excerpts = driver2.find_elements_by_xpath((\".//div[@class='search-result-content']/div[@class='search-result-excerpt']\"))\n",
    "        news_timestamps = driver2.find_elements_by_xpath((\".//div[@class='search-result-content']/h5[@class='search-result-timestamp']\"))\n",
    "\n",
    "        for title in news_titles:\n",
    "            titles.append(title.text)\n",
    "\n",
    "        for link in news_links:\n",
    "            links.append(link.get_attribute('href'))\n",
    "\n",
    "        for excerpt in news_excerpts:\n",
    "            excerpts.append(excerpt.text)\n",
    "\n",
    "        for timestamp in news_timestamps:\n",
    "            timestamps.append(timestamp.text)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"title\": titles,\n",
    "            \"link\": links,\n",
    "            \"excerpt\": excerpts,\n",
    "            \"timestamp\":timestamps\n",
    "        })    \n",
    "\n",
    "        clean_article_body = []\n",
    "        for i in links:\n",
    "            response=requests.get(i)\n",
    "            #print(response.content)\n",
    "            contents=response.content\n",
    "            alltext = BeautifulSoup(contents, \"lxml\")\n",
    "            #print(alltext)\n",
    "\n",
    "            article_body=[]\n",
    "            results = alltext.find_all('p', attrs={'class':'Paragraph-paragraph-2Bgue ArticleBody-para-TD_9x'})\n",
    "            for d in results:\n",
    "            #if title is not None:\n",
    "                article_body.append(d.text.strip())\n",
    "                cleantext = BeautifulSoup(str(article_body), \"lxml\").text\n",
    "                clean_txt = re.sub(r'[?|$|.|!]',r'',cleantext)\n",
    "            clean_article_body.append(clean_txt)\n",
    "\n",
    "        df_body = pd.DataFrame({\n",
    "            \"link\": links,\n",
    "            \"body\": clean_article_body\n",
    "        })    \n",
    "\n",
    "        dfinal = df.merge(df_body, on=\"link\", how = 'inner')\n",
    "        company = company.replace(\"+\",\" \").replace(\"%26\",\"&\").replace(\"%27\",\"'\")\n",
    "        dfinal.to_excel(writer, re.sub('[^A-Za-z0-9 _]*', '', company), index=False)\n",
    "        driver2.close()\n",
    "        ct = datetime.datetime.now()\n",
    "        print(company+\"current time:-\", ct)\n",
    "\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data in loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limitations, scraping is done in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter in range(5):\n",
    "    excel_path = \"../../../Working/Extracted/Scraped_Data_\" + str(counter+1) + \".xlsx\"\n",
    "    end_index = 40 * (counter + 1)\n",
    "    start_index = end_index - 40\n",
    "    if start_index != 0: start_index += 1\n",
    "    extract_to_excel(excel_path, name_url, start_index, end_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "li = []\n",
    "\n",
    "for file_counter in range(1,6):\n",
    "    file_name = '../../../Working/Extracted/Scraped_Data_' + str(file_counter) + '.xlsx'\n",
    "    wb = load_workbook(file_name, read_only=True)\n",
    "    print(file_name)\n",
    "    for sheet_name in wb.sheetnames:\n",
    "        try:\n",
    "            df = pd.read_excel(file_name, sheet_name, index_col=None)\n",
    "            df.insert(len(df.columns), 'company', sheet_name.strip())\n",
    "            li.append(df)\n",
    "        except:\n",
    "            print('Error occured in -> Filename : \"{0}\", Sheetname : \"{1}\"'.format(file_name, sheet_name))\n",
    "            continue\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_excel('../../DataSet/Final.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
